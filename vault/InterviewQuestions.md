# BERT related Interview topics 

1. About `[CLS]` token
What is the `[CLS]` token? Why is it at the start of the sequence? What is it used for? What are the alternatives? 

2. About tokenization
How does tokenization works in BERT? Name other tokenization techniques. What's the tokenization in PhoBert?

3. About accumulation grad updates
What is it for? How does it work?

4. How's *layer normalization* in BERT works? How is it compared to batch normalization
