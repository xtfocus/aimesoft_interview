# Tokenization

Tokenization is the conversion from a sequence into tokens - bite-sized items to be fed to language models. It's the very first processor in every language model.

There are several tokenization techniques: 
- Wordpiece (used by BERT, among others)
- Sentencepiece
- Byte-Pair Encoding (BPE)

By the time I made this note, Andrej Karpathy made a [video](https://www.youtube.com/watch?v=zduSFxRajkE) on how to build GPT's tokenizer.

# Reference
[[Mojo Future's guide]]
[Hugging Face's summary](https://huggingface.co/docs/transformers/tokenizer_summary)
